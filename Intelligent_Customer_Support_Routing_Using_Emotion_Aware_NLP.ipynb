{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intelligent Customer Support Routing Using Emotion-Aware NLP"
      ],
      "metadata": {
        "id": "My6k7KbEvudV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "Libraries required for data processing, visualization, and modeling."
      ],
      "metadata": {
        "id": "evXWrLl4p_36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csisElUANQ1r"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "\n",
        "# Install missing libraries\n",
        "!pip install emoji\n",
        "!pip install sentence-transformers umap-learn imblearn\n",
        "\n",
        "# Core\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# NLP / Data\n",
        "import nltk\n",
        "import emoji\n",
        "from datasets import load_dataset\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Embeddings / Dimensionality reduction\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "\n",
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Model selection / evaluation\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    f1_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "\n",
        "# Imbalance handling\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeWSGZ9vNWJ5"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "Load the GoEmotions dataset and convert it into a workable DataFrame format."
      ],
      "metadata": {
        "id": "cOBQwkqJqQUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpgFo_QuNdx0"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"go_emotions\", \"simplified\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzJrsZ6xNrsv"
      },
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_val = pd.DataFrame(dataset['validation'])\n",
        "df_test = pd.DataFrame(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt2CKr8aNzXm"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "crucial_words = {'not', 'no', 'nor', 'but', 'however', 'although', 'very', 'never'}\n",
        "final_stop_words = stop_words - crucial_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfypU2kBN5dv"
      },
      "outputs": [],
      "source": [
        "contractions = {\n",
        "    \"didn't\": \"did not\", \"don't\": \"do not\", \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\", \"couldn't\": \"could not\", \"doesn't\": \"does not\",\n",
        "    \"haven't\": \"have not\", \"isn't\": \"is not\", \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\", \"shouldn't\": \"should not\", \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\", \"i'm\": \"i am\", \"you're\": \"you are\", \"he's\": \"he is\",\n",
        "    \"it's\": \"it is\", \"we're\": \"we are\", \"they're\": \"they are\", \"let's\": \"let us\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovkpFPv_XDfj"
      },
      "source": [
        "## Data Cleaning\n",
        "Prepare text data for modeling by removing noise while preserving emotional signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um8EoaltN7y_"
      },
      "outputs": [],
      "source": [
        "def clean_text_professional(text):\n",
        "    # 1. Bot/noise removal\n",
        "    if text in [\"[deleted]\", \"[removed]\"] or pd.isna(text):\n",
        "        return \"\"\n",
        "    # 2. Lowercase\n",
        "    text = text.lower()\n",
        "    # 3. Expansion of Contractions\n",
        "    for word, replacement in contractions.items():\n",
        "        text = text.replace(word, replacement)\n",
        "    # 4. Emoji Handling\n",
        "    # We convert ðŸ˜¡ to \":enraged_face:\" so the model can read it as a word.\n",
        "    text = emoji.demojize(text)\n",
        "    # 5. Tokenization & Cleaning\n",
        "    text = re.sub(r\"[^a-z0-9\\s:_]\", \"\", text)\n",
        "    words = text.split()\n",
        "    # 6. Stopword Removal\n",
        "    filtered_words = [w for w in words if w not in final_stop_words]\n",
        "    return \" \".join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ist525adOXrb"
      },
      "outputs": [],
      "source": [
        "df_train['clean_text'] = df_train['text'].apply(clean_text_professional)\n",
        "df_val['clean_text'] = df_val['text'].apply(clean_text_professional)\n",
        "df_test['clean_text'] = df_test['text'].apply(clean_text_professional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jNQzJbKOcN3"
      },
      "outputs": [],
      "source": [
        "# Removing empty rows caused by cleaning\n",
        "df_train = df_train[df_train['clean_text'] != \"\"]\n",
        "df_val = df_val[df_val['clean_text'] != \"\"]\n",
        "df_test = df_test[df_test['clean_text'] != \"\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**\n",
        "Cleaning standardizes the text and reduces sparsity, which improves TF-IDF feature quality."
      ],
      "metadata": {
        "id": "G84TlN4SrJCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label Engineering"
      ],
      "metadata": {
        "id": "DG9d4VjFrSnF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48RGEivmXu8L"
      },
      "source": [
        "***The Logic:*** The raw dataset contains 27 granular emotions. For a Customer Support team, these are too specific. We aggregate them into 5 Actionable Priorities based on Business Risk:\n",
        "ðŸ”´ Angry (High Risk) $\\rightarrow$ Requires Senior Agent / Immediate De-escalation.\n",
        "ðŸŸ  Frustrated (Churn Risk) $\\rightarrow$ Requires Empathy.\n",
        "ðŸŸ£ Confused (Support Gap) $\\rightarrow$ Requires Education/FAQ updates.\n",
        "ðŸŸ¢ Satisfied (Retention) $\\rightarrow$ Validation of good service.\n",
        "ðŸ”µ Calm (Standard) $\\rightarrow$ Routine processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-QaSHJ8OfqZ"
      },
      "outputs": [],
      "source": [
        "# Getting the emotion names\n",
        "emotion_names = dataset['train'].features['labels'].feature.names\n",
        "# Defining the Mapping\n",
        "group_map = {\n",
        "    'anger': 'High Priority', 'annoyance': 'High Priority', 'disapproval': 'High Priority', 'disgust': 'High Priority',\n",
        "    'remorse': 'High Priority', 'sadness': 'High Priority', 'disappointment': 'High Priority', 'grief': 'High Priority', 'embarrassment': 'High Priority',\n",
        "    'confusion': 'Medium Priority', 'nervousness': 'Medium Priority', 'realization': 'Medium Priority', 'curiosity': 'Medium Priority', 'surprise': 'Medium Priority',\n",
        "    'neutral': 'Low Priority', 'approval': 'Low Priority', 'caring': 'Low Priority', 'joy': 'Low Priority', 'love': 'Low Priority', 'admiration': 'Low Priority',\n",
        "    'amusement': 'Low Priority', 'gratitude': 'Low Priority', 'optimism': 'Low Priority', 'relief': 'Low Priority', 'pride': 'Low Priority', 'excitement': 'Low Priority', 'desire': 'Low Priority'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usz1MsnlOkxh"
      },
      "outputs": [],
      "source": [
        "def get_business_priority(label_indices):\n",
        "    if not isinstance(label_indices, list) or not label_indices:\n",
        "        return \"Low Priority\"\n",
        "    current_emotions = [emotion_names[i] for i in label_indices]\n",
        "    groups = [group_map.get(e, 'Low Priority') for e in current_emotions]\n",
        "    if 'High Priority' in groups:\n",
        "        return 'High Priority'\n",
        "    elif 'Medium Priority' in groups:\n",
        "        return 'Medium Priority'\n",
        "    else:\n",
        "        return 'Low Priority'\n",
        "df_train['priority'] = df_train['labels'].apply(get_business_priority)\n",
        "df_val['priority'] = df_val['labels'].apply(get_business_priority)\n",
        "df_test['priority'] = df_test['labels'].apply(get_business_priority)\n",
        "\n",
        "print(df_train[['text', 'priority']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH9yDaXDUumH"
      },
      "outputs": [],
      "source": [
        "# Define the 5-Category Mapping\n",
        "# We map the 27 complex emotions into 5 Actionable Business Categories\n",
        "five_cat_map = {\n",
        "    # Category 1: ANGRY (Immediate Risk) - Requires Senior Agent\n",
        "    'anger': 'Angry', 'disgust': 'Angry', 'hate': 'Angry',\n",
        "\n",
        "    # Category 2: FRUSTRATED (Churn Risk) - Requires Empathy/Apology\n",
        "    'annoyance': 'Frustrated', 'disapproval': 'Frustrated', 'disappointment': 'Frustrated',\n",
        "    'remorse': 'Frustrated', 'sadness': 'Frustrated', 'grief': 'Frustrated', 'embarrassment': 'Frustrated',\n",
        "\n",
        "    # Category 3: CONFUSED (Support Needed) - Requires Clarity/FAQ\n",
        "    'confusion': 'Confused', 'curiosity': 'Confused', 'realization': 'Confused',\n",
        "    'surprise': 'Confused', 'nervousness': 'Confused', 'fear': 'Confused',\n",
        "\n",
        "    # Category 4: SATISFIED (Retention) - Requires Engagement/Upsell\n",
        "    'joy': 'Satisfied', 'excitement': 'Satisfied', 'pride': 'Satisfied',\n",
        "    'admiration': 'Satisfied', 'gratitude': 'Satisfied', 'love': 'Satisfied',\n",
        "    'relief': 'Satisfied', 'optimism': 'Satisfied', 'desire': 'Satisfied', 'amusement': 'Satisfied',\n",
        "\n",
        "    # Category 5: CALM (Routine) - Standard Processing\n",
        "    'neutral': 'Calm', 'approval': 'Calm', 'caring': 'Calm'\n",
        "}\n",
        "\n",
        "def get_five_category_label(label_indices):\n",
        "    # If multiple labels exist, prioritize the most severe one (Angry > Frustrated > Confused)\n",
        "    current_emotions = [emotion_names[i] for i in label_indices]\n",
        "    groups = [five_cat_map.get(e, 'Calm') for e in current_emotions]\n",
        "\n",
        "    # Hierarchy of severity\n",
        "    if 'Angry' in groups: return 'Angry'\n",
        "    if 'Frustrated' in groups: return 'Frustrated'\n",
        "    if 'Confused' in groups: return 'Confused'\n",
        "    if 'Satisfied' in groups: return 'Satisfied'\n",
        "    return 'Calm'\n",
        "\n",
        "# Applying the new mapping\n",
        "df_train['category_5'] = df_train['labels'].apply(get_five_category_label)\n",
        "df_val['category_5'] = df_val['labels'].apply(get_five_category_label)\n",
        "df_test['category_5'] = df_test['labels'].apply(get_five_category_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "Reducing 27 labels to 5 makes the classification task more stable and aligned with routing decisions.\n"
      ],
      "metadata": {
        "id": "eij5HKlVrwr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering: TF-IDF Representation\n",
        "\n",
        "Text is converted into numerical features using TF-IDF.\n",
        "\n",
        "Why TF-IDF:\n",
        "- Captures word importance\n",
        "- Handles sparse text efficiently\n",
        "- Strong baseline for linear classifiers\n"
      ],
      "metadata": {
        "id": "FqVPy91Twu9C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3KAOILo9hlR"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Discriminative Terms per 5-class label\n",
        "X_text = df_train['clean_text']\n",
        "y = df_train['category_5']\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    analyzer='char_wb',\n",
        "    ngram_range=(3,5),\n",
        "    max_features=50000\n",
        ")\n",
        "\n",
        "\n",
        "X = tfidf.fit_transform(X_text)\n",
        "terms = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "def top_tfidf_terms_for_label(label, top_k=20):\n",
        "    idx = (y == label).values\n",
        "    if idx.sum() == 0:\n",
        "        return pd.DataFrame(columns=[\"term\", \"score\"])\n",
        "\n",
        "    # Mean TF-IDF within label vs outside label (discriminative signal)\n",
        "    mean_in = X[idx].mean(axis=0).A1\n",
        "    mean_out = X[~idx].mean(axis=0).A1\n",
        "    delta = mean_in - mean_out\n",
        "\n",
        "    top_idx = np.argsort(delta)[-top_k:][::-1]\n",
        "    return pd.DataFrame({\"term\": terms[top_idx], \"score\": delta[top_idx]})\n",
        "\n",
        "for label in sorted(y.unique()):\n",
        "    display(top_tfidf_terms_for_label(label, top_k=20).style.set_caption(f\"Top discriminative TF-IDF terms for: {label}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis\n",
        "Understand class balance, language patterns, and signals relevant to priority routing.\n"
      ],
      "metadata": {
        "id": "EpDNu7EQwBLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Distribution Consistency Across Train, Validation, and Test Sets\n"
      ],
      "metadata": {
        "id": "5XM4OVc-tLzB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jABp3M_YYZjg"
      },
      "source": [
        "Before training, we must verify that our Test set is fair. If the Test set has a completely different distribution of emotions than the Training set (e.g., 90% Angry in Test vs. 10% in Train), our model metrics will be unreliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvJRuEoBUwuZ"
      },
      "outputs": [],
      "source": [
        "# Is it Balanced? (Train vs Test vs Val)\n",
        "# We need to see if the (Test) is harder than the (Train)\n",
        "df_train['split'] = 'Train'\n",
        "df_val['split'] = 'Validation'\n",
        "df_test['split'] = 'Test'\n",
        "\n",
        "combined_df = pd.concat([df_train[['category_5', 'split']],\n",
        "                         df_val[['category_5', 'split']],\n",
        "                         df_test[['category_5', 'split']]])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "props = combined_df.groupby('split')['category_5'].value_counts(normalize=True).unstack()\n",
        "props.plot(kind='bar', stacked=False, width=0.8, colormap='viridis', figsize=(12,6))\n",
        "\n",
        "plt.title(\"Distribution of 5 Categories across Splits (Normalized)\")\n",
        "plt.ylabel(\"Percentage of Dataset\")\n",
        "plt.xlabel(\"Dataset Split\")\n",
        "plt.legend(title=\"Emotion Category\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"INSIGHT CHECK: Look at the bars. Are the 'Angry' bars roughly the same height in Train, Val, and Test? If Test has 50% Angry but Train only has 10%, our model will fail.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "The emotion category proportions are nearly identical across Train, Validation, and Test sets.  \n",
        "This indicates that the data splitting preserved class distribution, ensuring fair model evaluation.  \n",
        "\n",
        "Because no split is disproportionately skewed (e.g., more Angry in Test), performance metrics will reliably reflect real deployment behavior.\n"
      ],
      "metadata": {
        "id": "-j1ziTfLtOXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top Emojis by Emotion Category\n"
      ],
      "metadata": {
        "id": "kL5AOI8FtZOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1BcWh0bVMH_"
      },
      "outputs": [],
      "source": [
        "#Question: \"Are emojis distinct markers for priority? Can we flag a ticket just because it has a 'skull' emoji?\"\n",
        "def extract_emojis(text):\n",
        "    # Our cleaning kept emojis in the format :emoji_name:\n",
        "    # This regex finds words wrapped in colons\n",
        "    return re.findall(r':[a-z_]+:', text)\n",
        "\n",
        "# Creating a column just for emojis for all dataframes\n",
        "df_train['emojis'] = df_train['clean_text'].apply(extract_emojis)\n",
        "df_val['emojis'] = df_val['clean_text'].apply(extract_emojis)\n",
        "df_test['emojis'] = df_test['clean_text'].apply(extract_emojis)\n",
        "\n",
        "# Emojis per 5 Categories\n",
        "# Which emojis signal \"Frustration\" vs \"Anger\"?\n",
        "\n",
        "emoji_counts_5 = {cat: Counter() for cat in ['Angry', 'Frustrated', 'Confused', 'Satisfied', 'Calm']}\n",
        "\n",
        "for _, row in df_train.iterrows():\n",
        "    cat = row['category_5']\n",
        "    if row['emojis']:\n",
        "        emoji_counts_5[cat].update(row['emojis'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(24, 5))\n",
        "categories = ['Angry', 'Frustrated', 'Confused', 'Satisfied', 'Calm']\n",
        "colors = ['#cc0000', '#ff9933', '#9933ff', '#33cc33', '#66b3ff'] # Custom Palette\n",
        "\n",
        "for i, cat in enumerate(categories):\n",
        "    common = emoji_counts_5[cat].most_common(5)\n",
        "    if common:\n",
        "        emo_df = pd.DataFrame(common, columns=['Emoji', 'Count'])\n",
        "        cat_size = (df_train['category_5'] == cat).sum()\n",
        "        emo_df['Rate_per_10k'] = (emo_df['Count'] / cat_size) * 10000\n",
        "\n",
        "        sns.barplot(x='Rate_per_10k', y='Emoji', data=emo_df, ax=axes[i], color=colors[i])\n",
        "        axes[i].set_xlabel(\"Rate per 10k messages\")\n",
        "\n",
        "        axes[i].set_title(f'{cat}')\n",
        "    else:\n",
        "        axes[i].text(0.5, 0.5, \"No Emojis\", ha='center')\n",
        "\n",
        "plt.suptitle(\"Top Emojis by Emotion Category\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "Emojis show distinct patterns across emotion categories.  \n",
        "Negative emotions (Angry, Frustrated) frequently include intense or expressive emojis,  \n",
        "while Calm and Satisfied categories contain more positive or neutral emojis.\n",
        "\n",
        "This confirms that emojis carry strong emotional signal and should be preserved during preprocessing, as they can improve classification performance and priority detection.\n"
      ],
      "metadata": {
        "id": "M38uPxZutbsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Phrases in Negative Messages"
      ],
      "metadata": {
        "id": "fbKhj-8jtonj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13DNk02WVRyd"
      },
      "outputs": [],
      "source": [
        "# Deep Reasoning Analysis\n",
        "\n",
        "def plot_top_trigrams(category, color, title):\n",
        "    text_data = df_train[df_train['category_5'] == category]['clean_text']\n",
        "\n",
        "    # Using Trigrams (ngram_range=(3,3)) to capture \"Context-Reason-Object\"\n",
        "    # min_df=2 removes rare typos to focus on common reasons\n",
        "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english', min_df=2).fit(text_data)\n",
        "    bag_of_words = vec.transform(text_data)\n",
        "    doc_freq = (bag_of_words > 0).sum(axis=0)  # number of docs containing trigram\n",
        "    words_freq = [(word, doc_freq[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "\n",
        "\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:10]\n",
        "    df_trigram = pd.DataFrame(words_freq, columns=['Phrase', 'Doc_Freq'])\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x='Doc_Freq', y='Phrase', data=df_trigram, color=color)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Visualizing the Negative/Confused Categories (Where we need reasons)\n",
        "print(\"Analyzing the REASONS for Anger (Trigrams)...\")\n",
        "plot_top_trigrams('Angry', '#cc0000', \"Why are they Angry? (Common Phrases)\")\n",
        "\n",
        "print(\"Analyzing the REASONS for Frustration...\")\n",
        "plot_top_trigrams('Frustrated', '#ff9933', \"Why are they Frustrated?\")\n",
        "\n",
        "print(\"Analyzing the REASONS for Confusion...\")\n",
        "plot_top_trigrams('Confused', '#9933ff', \"What are they Confused about?\")\n",
        "\n",
        "# \"Causal\" Keyword Mining\n",
        "# Find sentences that explicitly state the reason using \"because\"\n",
        "print(\"\\n--- Causal Snippets (Mining 'Because') ---\")\n",
        "\n",
        "def get_causal_snippets(category, num_samples=5):\n",
        "    # Filter for the category\n",
        "    subset = df_train[df_train['category_5'] == category]\n",
        "    # Find rows containing \"because\"\n",
        "    reasons = subset[subset['text'].str.contains(\" because \", case=False, na=False)]['text']\n",
        "\n",
        "    print(f\"\\n[{category.upper()}] Customer Explanations:\")\n",
        "    if len(reasons) > 0:\n",
        "        for text in reasons.sample(min(len(reasons), num_samples)):\n",
        "            print(f\" -> \\\"{text}\\\"\")\n",
        "    else:\n",
        "        print(\" -> No explicit 'because' statements found.\")\n",
        "\n",
        "get_causal_snippets('Angry')\n",
        "get_causal_snippets('Frustrated')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "Angry messages contain aggressive or hostile phrasing, indicating direct dissatisfaction or blame.  \n",
        "These expressions reflect high emotional intensity and support classifying Angry messages as high-priority for routing.\n",
        "\n",
        "Frustrated messages often include complaint-driven or apologetic phrases, suggesting ongoing dissatisfaction rather than explosive anger.  \n",
        "This indicates operational urgency, but typically lower intensity than Angry, supporting a medium-to-high priority assignment.\n",
        "\n",
        "Confused messages contain clarification-seeking or uncertainty-related phrases.  \n",
        "These indicate a need for guidance rather than escalation, making them suitable for routing to support or help-desk teams instead of high-priority complaint handling."
      ],
      "metadata": {
        "id": "_8kLj_XHtxhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Frequent Words by Emotion Category\n"
      ],
      "metadata": {
        "id": "aysq1JiRt7t-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9qJCIlYV0oa"
      },
      "outputs": [],
      "source": [
        "# Visualizing Top Words per Category\n",
        "\n",
        "def plot_top_words(category, color, ax):\n",
        "    # 1. Filter data for this specific category\n",
        "    text_data = df_train[df_train['category_5'] == category]['clean_text']\n",
        "\n",
        "    # 2. Safety Check: If data is empty, we skip\n",
        "    if text_data.empty:\n",
        "        return\n",
        "\n",
        "    # 3. Vectorize (Count raw frequency)\n",
        "    # We add 'custom_stop_words' to remove generic fluff like \"im\", \"just\", \"like\"\n",
        "    # This reveals the REAL topics (e.g., \"game\", \"money\", \"time\")\n",
        "    custom_stop = list(final_stop_words) + ['im', 'just', 'like', 'feel', 'dont', 'know', 'really', 'want', 'get']\n",
        "\n",
        "    vec = CountVectorizer(stop_words=custom_stop, max_features=20).fit(text_data)\n",
        "    bag_of_words = vec.transform(text_data)\n",
        "    doc_freq = (bag_of_words > 0).sum(axis=0)\n",
        "    words_freq = [(word, doc_freq[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "\n",
        "\n",
        "    # 4. Sort and get Top 15\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:15]\n",
        "    df_words = pd.DataFrame(words_freq, columns=['Word', 'Doc_Freq'])\n",
        "\n",
        "    # 5. Plot\n",
        "    sns.barplot(x='Doc_Freq', y='Word', data=df_words, ax=ax, color=color)\n",
        "    ax.set_title(f\"Top Words in '{category}'\")\n",
        "    ax.set_xlabel(\"Docs containing word\")\n",
        "    ax.set_ylabel(\"\")\n",
        "\n",
        "# Create a huge canvas for 5 plots\n",
        "fig, axes = plt.subplots(1, 5, figsize=(25, 6))\n",
        "categories = ['Angry', 'Frustrated', 'Confused', 'Satisfied', 'Calm']\n",
        "colors = ['#cc0000', '#ff9933', '#9933ff', '#33cc33', '#66b3ff'] # Red, Orange, Purple, Green, Blue\n",
        "\n",
        "# Loop through and plot\n",
        "for i, cat in enumerate(categories):\n",
        "    print(f\"Generating plot for {cat}...\")\n",
        "    plot_top_words(cat, colors[i], axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "Each emotion category exhibits distinct vocabulary patterns.  \n",
        "\n",
        "- Angry messages contain strong negative and offensive terms, reflecting high emotional intensity.\n",
        "- Frustrated messages include dissatisfaction markers (e.g., â€œsorryâ€, â€œbadâ€), suggesting ongoing issues.\n",
        "- Confused messages contain uncertainty-related words (e.g., â€œwouldâ€, â€œthinkâ€), indicating clarification needs.\n",
        "- Satisfied messages show positive expressions (e.g., â€œloveâ€, â€œthanksâ€), reflecting resolution or approval.\n",
        "- Calm messages contain neutral conversational language.\n",
        "\n",
        "These separable word patterns support the use of TF-IDF with linear classifiers, as the categories are lexically distinguishable.\n"
      ],
      "metadata": {
        "id": "bXMLoWQ9uALm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Priority Level Distribution Across Train, Validation, and Test Sets\n"
      ],
      "metadata": {
        "id": "_1FBCwN1uEP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3NdXmeGOxGS"
      },
      "outputs": [],
      "source": [
        "#Distribution Consistency Check\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
        "order = ['High Priority', 'Medium Priority', 'Low Priority']\n",
        "palette = {'High Priority': '#ff4d4d', 'Medium Priority': '#ffcc00', 'Low Priority': '#66b3ff'}\n",
        "sns.countplot(x='priority', data=df_train, order=order, palette=palette, ax=axes[0])\n",
        "axes[0].set_title('Training Data Distribution')\n",
        "sns.countplot(x='priority', data=df_val, order=order, palette=palette, ax=axes[1])\n",
        "axes[1].set_title('Validation Data Distribution')\n",
        "sns.countplot(x='priority', data=df_test, order=order, palette=palette, ax=axes[2])\n",
        "axes[2].set_title('Test Data Distribution')\n",
        "plt.show()\n",
        "print(\"Insight: Ensure the bars look roughly similar across all three charts. If Test has way more High Priority than Train, our model will fail.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "Low-priority messages dominate across all splits, while High-priority messages form a smaller proportion of the dataset.\n",
        "\n",
        "This imbalance means:\n",
        "- A model could achieve high accuracy by favoring Low priority.\n",
        "- Macro-F1 or class-sensitive evaluation is necessary.\n",
        "- Misclassifying High-priority tickets would have a larger business impact despite their lower frequency.\n"
      ],
      "metadata": {
        "id": "7Pt8VWdluLtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Phrases in High-Priority Tickets\n"
      ],
      "metadata": {
        "id": "uCrnRHo8uOWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X42hz21oO9fC"
      },
      "outputs": [],
      "source": [
        "def plot_top_bigrams(text_data, title):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(text_data)\n",
        "    bag_of_words = vec.transform(text_data)\n",
        "    doc_freq = (bag_of_words > 0).sum(axis=0)\n",
        "    words_freq = [(word, doc_freq[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    df_bigram = pd.DataFrame(words_freq, columns=['Bigram', 'Doc_Freq'])\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x='Doc_Freq', y='Bigram', data=df_bigram, palette='Reds_r')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Docs containing bigram\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nAnalyzing what creates High Priority tickets...\")\n",
        "high_priority_text = df_train[df_train['priority'] == 'High Priority']['clean_text']\n",
        "plot_top_bigrams(high_priority_text, \"Top 10 Phrases in High Priority Tickets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "High-priority tickets frequently contain phrases expressing dissatisfaction, concern, or urgency (e.g., â€œfeel badâ€, â€œreally badâ€, â€œdonâ€™t likeâ€).  \n",
        "\n",
        "These patterns confirm that priority assignment is strongly tied to emotional intensity.  \n",
        "It also suggests that phrase-level features (bigrams) improve detection of urgent cases beyond single-word signals.\n"
      ],
      "metadata": {
        "id": "daZRGpc1uUnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emoji Distribution Across Priority Levels\n"
      ],
      "metadata": {
        "id": "sVf5tFH9ucBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DE410ub5QBRR"
      },
      "outputs": [],
      "source": [
        "# Count emojis per Priority Group\n",
        "emoji_counts = {\n",
        "    'High Priority': Counter(),\n",
        "    'Medium Priority': Counter(),\n",
        "    'Low Priority': Counter()\n",
        "}\n",
        "\n",
        "for _, row in df_train.iterrows():\n",
        "    priority = row['priority']\n",
        "    if row['emojis']:\n",
        "        emoji_counts[priority].update(row['emojis'])\n",
        "\n",
        "# Visualize Top Emojis\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "priorities = ['High Priority', 'Medium Priority', 'Low Priority']\n",
        "colors = ['#ff4d4d', '#ffcc00', '#66b3ff'] # Red, Yellow, Blue\n",
        "\n",
        "for i, priority in enumerate(priorities):\n",
        "    # Get top 5 emojis\n",
        "    common = emoji_counts[priority].most_common(5)\n",
        "    if common:\n",
        "        emo_df = pd.DataFrame(common, columns=['Emoji', 'Count'])\n",
        "        prio_size = (df_train['priority'] == priority).sum()\n",
        "        emo_df['Rate_per_10k'] = (emo_df['Count'] / prio_size) * 10000\n",
        "        sns.barplot(x='Rate_per_10k', y='Emoji', data=emo_df, ax=axes[i], color=colors[i])\n",
        "        axes[i].set_xlabel(\"Rate per 10k messages\")\n",
        "    else:\n",
        "        axes[i].text(0.5, 0.5, \"No Emojis Found\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"INSIGHT: If specific emojis like ':face_with_symbols_on_mouth:' appear ONLY in High Priority, we can create a 'Flag' feature for them.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "High-priority tickets show stronger presence of emotionally intense emojis (e.g., crying, loud crying),  \n",
        "while lower-priority tickets contain more neutral or positive emojis (e.g., hearts, laughter).\n",
        "\n",
        "This suggests emoji intensity correlates with urgency.  \n",
        "Emoji-based features could serve as early indicators for high-priority routing decisions.\n"
      ],
      "metadata": {
        "id": "cmwq_dSeue8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Message Length vs. Priority Level\n"
      ],
      "metadata": {
        "id": "lDhAz8hNui7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbwH6YiHQIig"
      },
      "outputs": [],
      "source": [
        "#Question: \"Do angry customers write long essays (rants) while happy customers write short 'Thank yous'?\"\n",
        "# Calculate word count\n",
        "df_train['word_count'] = df_train['clean_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='priority', y='word_count', data=df_train,\n",
        "            order=['High Priority', 'Medium Priority', 'Low Priority'],\n",
        "            palette={'High Priority': '#ff4d4d', 'Medium Priority': '#ffcc00', 'Low Priority': '#66b3ff'})\n",
        "\n",
        "plt.title(\"Message Length vs. Priority (The 'Rant' Factor)\")\n",
        "plt.ylim(0, 50) # Limit y-axis to ignore extreme outliers for better readability\n",
        "plt.show()\n",
        "print(\"INSIGHT: Look at the median line (the line inside the box). If 'High Priority' is higher, it confirms that angry customers write more.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "Message length distributions are relatively similar across priority levels, with only slight variation in medians and spread.\n",
        "\n",
        "This suggests that urgency is not strongly determined by message length alone.  \n",
        "Emotional content and wording patterns are more informative features than text length for routing decisions.\n"
      ],
      "metadata": {
        "id": "R4SHs6-MuqrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Drivers of High-Priority and Medium-Priority Tickets\n"
      ],
      "metadata": {
        "id": "9ewTaO1tuypb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glzJVSbbQYfT"
      },
      "outputs": [],
      "source": [
        "#Question: \"We know they are angry. But why? Is it 'Bad Service' or 'Late Delivery'?\"\n",
        "\n",
        "def plot_distinctive_bigrams(priority_group, color):\n",
        "    # Filter text for this specific group\n",
        "    text_data = df_train[df_train['priority'] == priority_group]['clean_text']\n",
        "\n",
        "    # Use Bigrams (2 words)\n",
        "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(text_data)\n",
        "    bag_of_words = vec.transform(text_data)\n",
        "    doc_freq = (bag_of_words > 0).sum(axis=0)\n",
        "    words_freq = [(word, doc_freq[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "\n",
        "\n",
        "    # Sort and take top 10\n",
        "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)[:10]\n",
        "    df_bigram = pd.DataFrame(words_freq, columns=['Phrase', 'Doc_Freq'])\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(x='Doc_Freq', y='Phrase', data=df_bigram, color=color)\n",
        "    plt.xlabel(\"Docs containing phrase\")\n",
        "    plt.title(f\"Why are they {priority_group}? (Top Phrases)\")\n",
        "    plt.show()\n",
        "\n",
        "# Run for High and Medium (Low is usually less actionable)\n",
        "print(f\"\\n--- Investigating High Priority Drivers ---\")\n",
        "plot_distinctive_bigrams('High Priority', '#ff4d4d')\n",
        "\n",
        "print(f\"\\n--- Investigating Medium Priority Drivers ---\")\n",
        "plot_distinctive_bigrams('Medium Priority', '#ffcc00')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "High-priority tickets contain strong dissatisfaction phrases (e.g., â€œfeel badâ€, â€œreally badâ€, â€œdonâ€™t likeâ€), indicating emotional intensity and negative experience.\n",
        "\n",
        "These expressions justify escalation and support mapping high-intensity language directly to urgent routing decisions.\n",
        "\n",
        "Medium-priority tickets contain uncertainty and clarification-related phrases (e.g., â€œdonâ€™t knowâ€, â€œfeel likeâ€, â€œlooks likeâ€).\n",
        "\n",
        "This suggests these messages reflect unresolved issues or confusion rather than direct anger, making them suitable for standard support handling rather than escalation.\n",
        "\n"
      ],
      "metadata": {
        "id": "gunL-WD0u3XL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emotion Co-occurrence Patterns\n"
      ],
      "metadata": {
        "id": "9hctL_2RvElC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdu3SCR2WM-t"
      },
      "outputs": [],
      "source": [
        "# 1. Convert list of indices to list of names\n",
        "# We use the raw 27 emotions here to see fine-grained detail\n",
        "df_train['emotion_names'] = df_train['labels'].apply(lambda x: [emotion_names[i] for i in x])\n",
        "\n",
        "# 2. One-Hot Encode\n",
        "mlb = MultiLabelBinarizer()\n",
        "emotion_matrix = mlb.fit_transform(df_train['emotion_names'])\n",
        "emotion_df = pd.DataFrame(emotion_matrix, columns=mlb.classes_)\n",
        "E = emotion_df.values.astype(int)  # shape: [n_samples, n_emotions]\n",
        "labels = emotion_df.columns.tolist()\n",
        "\n",
        "# Raw co-occurrence counts\n",
        "co_counts = E.T @ E\n",
        "co_counts_df = pd.DataFrame(co_counts, index=labels, columns=labels)\n",
        "\n",
        "# Normalize to \"given A, probability of B\" = P(B|A)\n",
        "row_sums = co_counts_df.sum(axis=1).replace(0, 1)\n",
        "co_prob_df = co_counts_df.div(row_sums, axis=0)\n",
        "\n",
        "# Show top pairings (exclude diagonal)\n",
        "pairs = []\n",
        "for i in range(len(labels)):\n",
        "    for j in range(i+1, len(labels)):\n",
        "        pairs.append((labels[i], labels[j], co_counts_df.iat[i, j]))\n",
        "top_pairs = sorted(pairs, key=lambda x: x[2], reverse=True)[:20]\n",
        "print(\"Top co-occurring emotion pairs (by count):\")\n",
        "for a, b, c in top_pairs:\n",
        "    print(f\"{a} + {b}: {c}\")\n",
        "\n",
        "# Plot a focused heatmap for a subset\n",
        "focus = ['anger','annoyance','disapproval','disgust','sadness','disappointment','remorse',\n",
        "         'confusion','curiosity','fear','nervousness','surprise','neutral','joy','love','gratitude','optimism']\n",
        "\n",
        "focus = [x for x in focus if x in labels]\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(co_prob_df.loc[focus, focus], cmap='Blues', annot=False)\n",
        "plt.title(\"Emotion Co-occurrence (Row-normalized: P(col | row))\")\n",
        "plt.xlabel(\"Emotion B\")\n",
        "plt.ylabel(\"Emotion A\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "Certain emotions frequently co-occur (e.g., anger with annoyance or disapproval), while positive emotions cluster together (joy, love, gratitude).\n",
        "\n",
        "This overlap explains why some categories may be difficult to separate perfectly during classification.  \n",
        "Grouping fine-grained emotions into broader operational categories reduces this ambiguity and improves routing stability.\n"
      ],
      "metadata": {
        "id": "5pRvaP0avLhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UMAP Projection of Sentence Embeddings by Emotion Category\n"
      ],
      "metadata": {
        "id": "4ZB_frr3vUYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsJ2xjCO-Oyg"
      },
      "outputs": [],
      "source": [
        "# Embedding Visualization\n",
        "N = 8000\n",
        "viz_df = df_train.sample(min(N, len(df_train)), random_state=42).copy()\n",
        "\n",
        "texts = viz_df['text'].tolist()\n",
        "labels_5 = viz_df['category_5'].tolist()\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "emb = model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
        "\n",
        "reducer = umap.UMAP(n_neighbors=25, min_dist=0.1, metric=\"cosine\", random_state=42)\n",
        "xy = reducer.fit_transform(emb)\n",
        "\n",
        "viz_df[\"x\"] = xy[:, 0]\n",
        "viz_df[\"y\"] = xy[:, 1]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for lab in sorted(viz_df[\"category_5\"].unique()):\n",
        "    subset = viz_df[viz_df[\"category_5\"] == lab]\n",
        "    plt.scatter(subset[\"x\"], subset[\"y\"], s=8, alpha=0.6, label=lab)\n",
        "\n",
        "plt.title(\"UMAP of Sentence Embeddings (MiniLM) colored by 5-category label\")\n",
        "plt.xlabel(\"UMAP-1\")\n",
        "plt.ylabel(\"UMAP-2\")\n",
        "plt.legend(markerscale=2)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:**  \n",
        "The emotion categories show partial clustering but also significant overlap in the embedding space.\n",
        "\n",
        "This indicates:\n",
        "- Emotional signals are present and somewhat learnable.\n",
        "- Boundaries between categories (e.g., Angry vs. Frustrated) are not perfectly separable.\n",
        "- Some classification errors are expected due to semantic overlap.\n",
        "\n",
        "This reinforces the need for robust evaluation and possibly more expressive models for finer separation.\n"
      ],
      "metadata": {
        "id": "zX2wY6khvYq1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB6UTYM1WPWg"
      },
      "outputs": [],
      "source": [
        "# In GoEmotions, 'neutral' is a label, but sometimes lists are empty []\n",
        "df_train['label_count'] = df_train['labels'].apply(len)\n",
        "unlabeled_rows = df_train[df_train['label_count'] == 0]\n",
        "\n",
        "print(f\"Total Unlabeled Rows: {len(unlabeled_rows)}\")\n",
        "\n",
        "if len(unlabeled_rows) > 0:\n",
        "    print(\"\\n--- Sample of Unlabeled Content ---\")\n",
        "    # Let's see what these 'Ghost' tickets look like\n",
        "    for text in unlabeled_rows['text'].head(5):\n",
        "        print(f\" -> {text}\")\n",
        "    print(\"\\nDECISION: If these look like junk, we drop them. If they look real, we might map them to 'Neutral'.\")\n",
        "else:\n",
        "    print(\"Great! All rows have at least one label. No data quality issues here.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Evaluation\n",
        "\n",
        "In this phase, text data is transformed using TF-IDF features and evaluated using linear classifiers.\n",
        "\n",
        "Logistic Regression and Linear SVM are compared to determine which model better separates the operational emotion categories.\n",
        "\n",
        "Evaluation focuses on Macro-F1 to ensure balanced performance across all priority-sensitive classes.\n"
      ],
      "metadata": {
        "id": "B-EylatWxcVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zFhmVDfeiiR"
      },
      "outputs": [],
      "source": [
        "X_train = df_train['clean_text']\n",
        "y_train = df_train['category_5']\n",
        "print(\"Train distribution (raw):\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nTrain distribution (percent):\")\n",
        "print((y_train.value_counts(normalize=True) * 100).round(2))\n",
        "X_val = df_val['clean_text']\n",
        "y_val = df_val['category_5']\n",
        "\n",
        "X_test = df_test['clean_text']\n",
        "y_test = df_test['category_5']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkyPCKJXot6n"
      },
      "outputs": [],
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bk3OTz8xe6et"
      },
      "outputs": [],
      "source": [
        "sampler = RandomOverSampler(random_state=42)\n",
        "\n",
        "pipeline_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
        "    ('sampler', sampler),\n",
        "    # Oversampling already balances the training set, so class_weight is optional.\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_svm = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
        "    ('sampler', sampler),\n",
        "    ('clf', LinearSVC(dual='auto', random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_sgd = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2))),\n",
        "    ('sampler', sampler),\n",
        "    ('clf', SGDClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "pipeline_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=50000, ngram_range=(1,2), norm=None)),\n",
        "    ('sampler', sampler),\n",
        "    ('clf', ComplementNB(alpha=0.5))\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r9X-MsKfDrm"
      },
      "outputs": [],
      "source": [
        "print(\"Training Logistic Regression...\")\n",
        "pipeline_lr.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred_lr = pipeline_lr.predict(X_train)\n",
        "train_f1_lr = f1_score(y_train, y_train_pred_lr, average='macro')\n",
        "\n",
        "y_val_pred_lr = pipeline_lr.predict(X_val)\n",
        "val_f1_lr = f1_score(y_val, y_val_pred_lr, average='macro')\n",
        "\n",
        "print(f\"LogReg Macro-F1 | Train: {train_f1_lr:.4f} | Val: {val_f1_lr:.4f}\")\n",
        "score_lr = val_f1_lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lHMWxuzfH1I"
      },
      "outputs": [],
      "source": [
        "print(\"Training Linear SVM...\")\n",
        "pipeline_svm.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred_svm = pipeline_svm.predict(X_train)\n",
        "train_f1_svm = f1_score(y_train, y_train_pred_svm, average='macro')\n",
        "\n",
        "y_val_pred_svm = pipeline_svm.predict(X_val)\n",
        "val_f1_svm = f1_score(y_val, y_val_pred_svm, average='macro')\n",
        "\n",
        "print(f\"SVM Macro-F1    | Train: {train_f1_svm:.4f} | Val: {val_f1_svm:.4f}\")\n",
        "score_svm = val_f1_svm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAV1cmZQ0VvL"
      },
      "outputs": [],
      "source": [
        "X_tmp = pipeline_svm.named_steps['tfidf'].fit_transform(X_train)\n",
        "y_tmp = y_train.copy()\n",
        "X_res, y_res = RandomOverSampler(random_state=42).fit_resample(X_tmp, y_tmp)\n",
        "print(\"\\nAfter oversampling (should be equal counts):\")\n",
        "print(pd.Series(y_res).value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L91QKPz4fRNE"
      },
      "outputs": [],
      "source": [
        "if score_svm > score_lr:\n",
        "    best_model = pipeline_svm\n",
        "    model_name = \"Linear SVM\"\n",
        "else:\n",
        "    best_model = pipeline_lr\n",
        "    model_name = \"Logistic Regression\"\n",
        "\n",
        "print(f\"\\n WINNER: {model_name} (Proceeding to Optimization)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3ewIYm4fYWP"
      },
      "outputs": [],
      "source": [
        "# Better hyperparameter tuning\n",
        "# Tunes TF-IDF + model together, uses stronger CV, and is faster than large GridSearch.\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Candidate 1: Linear SVM\n",
        "param_dist_svm = {\n",
        "    'tfidf__max_features': [5000, 10000, 20000],\n",
        "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
        "    'tfidf__min_df': [2, 3, 5],\n",
        "    'tfidf__max_df': [0.8, 0.9, 1.0],\n",
        "    'tfidf__sublinear_tf': [True, False],\n",
        "    'clf__C': [0.1, 0.3, 1, 3, 10]\n",
        "}\n",
        "\n",
        "search_svm = RandomizedSearchCV(\n",
        "    pipeline_svm,\n",
        "    param_distributions=param_dist_svm,\n",
        "    n_iter=30,\n",
        "    cv=cv,\n",
        "    scoring='f1_macro',\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "search_svm.fit(X_train, y_train)\n",
        "\n",
        "best_svm = search_svm.best_estimator_\n",
        "best_svm_score = search_svm.best_score_\n",
        "print(\"Best SVM params:\", search_svm.best_params_)\n",
        "print(\"Best SVM CV Macro-F1:\", best_svm_score)\n",
        "\n",
        "\n",
        "# Candidate 2: SGDClassifier\n",
        "param_dist_sgd = {\n",
        "    'tfidf__max_features': [5000, 10000, 20000],\n",
        "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
        "    'tfidf__min_df': [2, 3, 5],\n",
        "    'tfidf__max_df': [0.8, 0.9, 1.0],\n",
        "    'tfidf__sublinear_tf': [True, False],\n",
        "    'clf__loss': ['hinge', 'log_loss'],\n",
        "    'clf__alpha': [1e-5, 1e-4, 1e-3],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "    'clf__max_iter': [3000]\n",
        "}\n",
        "\n",
        "search_sgd = RandomizedSearchCV(\n",
        "    pipeline_sgd,\n",
        "    param_distributions=param_dist_sgd,\n",
        "    n_iter=30,\n",
        "    cv=cv,\n",
        "    scoring='f1_macro',\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "search_sgd.fit(X_train, y_train)\n",
        "\n",
        "best_sgd = search_sgd.best_estimator_\n",
        "best_sgd_score = search_sgd.best_score_\n",
        "print(\"Best SGD params:\", search_sgd.best_params_)\n",
        "print(\"Best SGD CV Macro-F1:\", best_sgd_score)\n",
        "\n",
        "\n",
        "# Choose winner by CV Macro-F1\n",
        "if best_sgd_score > best_svm_score:\n",
        "    final_model = best_sgd\n",
        "    print(\"WINNER: SGDClassifier\")\n",
        "else:\n",
        "    final_model = best_svm\n",
        "    print(\"WINNER: Linear SVM\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cstco9KSraun"
      },
      "outputs": [],
      "source": [
        "# Learning Curve (Macro-F1)\n",
        "\n",
        "\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    final_model,\n",
        "    X_train, y_train,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 6),\n",
        "    cv=cv,\n",
        "    scoring=\"f1_macro\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "val_mean = val_scores.mean(axis=1)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_sizes, train_mean, marker='o', label='Train Macro-F1')\n",
        "plt.plot(train_sizes, val_mean, marker='o', label='CV Macro-F1')\n",
        "plt.title(\"Learning Curve (Macro-F1)\")\n",
        "plt.xlabel(\"Training examples\")\n",
        "plt.ylabel(\"Macro-F1\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsV8ugnPf7K2"
      },
      "outputs": [],
      "source": [
        "y_pred = final_model.predict(X_test)\n",
        "\n",
        "# 1. Classification Report\n",
        "print(f\"--- FINAL REPORT: {model_name} ---\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "angry_recall = recall_score(y_test, y_pred, labels=['Angry'], average=None)[0]\n",
        "print(f\"Angry Recall (Risk Catch Rate): {angry_recall:.4f}\")\n",
        "\n",
        "# 2. Visual Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "# Normalize='true' gives percentages, which is easier to understand\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax, cmap='Blues', normalize='true')\n",
        "plt.title(\"Where does the model fail? (Normalized Confusion Matrix)\")\n",
        "plt.show()\n",
        "\n",
        "print(\"INSIGHT: Check the 'Angry' Box on the diagonal. If it's > 0.60, we are catching 60% of risks. In text analysis, that is often considered 'Good' for 5 classes.\")\n",
        "\n",
        "print(\"TEST Macro-F1:\", f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"TEST Weighted-F1:\", f1_score(y_test, y_pred, average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmiExG83gDgr"
      },
      "outputs": [],
      "source": [
        "# Extract the vocabulary\n",
        "feature_names = final_model.named_steps['tfidf'].get_feature_names_out()\n",
        "\n",
        "# Get model coefficients\n",
        "if model_name == \"Linear SVM\":\n",
        "    coefs = final_model.named_steps['clf'].coef_\n",
        "    classes = final_model.named_steps['clf'].classes_\n",
        "elif model_name == \"Logistic Regression\":\n",
        "    coefs = final_model.named_steps['clf'].coef_\n",
        "    classes = final_model.named_steps['clf'].classes_\n",
        "\n",
        "# Find the index for 'Angry'\n",
        "angry_index = list(classes).index('Angry')\n",
        "angry_coefs = coefs[angry_index]\n",
        "\n",
        "# Get Top 15 Words driving \"Angry\"\n",
        "top_indices = np.argsort(angry_coefs)[-15:]\n",
        "print(\"--- ðŸš¨ The 'Danger Words' (Top triggers for Angry) ---\")\n",
        "for i in top_indices:\n",
        "    print(f\"Word: '{feature_names[i]}' \\t Weight: {angry_coefs[i]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU0GjoAEgJp_"
      },
      "outputs": [],
      "source": [
        "def predict_priority(text):\n",
        "    # 1. Clean using Phase 1 logic\n",
        "    clean = clean_text_professional(text)\n",
        "\n",
        "    # 2. Predict\n",
        "    pred = final_model.predict([clean])[0]\n",
        "\n",
        "    # 3. Business Routing Logic\n",
        "    actions = {\n",
        "        'Angry': \"ðŸ”´ URGENT: Route to Senior Agent (Risk Team)\",\n",
        "        'Frustrated': \"ðŸŸ  HIGH: Route to Retention Team\",\n",
        "        'Confused': \"ðŸŸ£ MEDIUM: Route to Support/Education\",\n",
        "        'Satisfied': \"ðŸŸ¢ LOW: Auto-Reply 'Thank you'\",\n",
        "        'Calm': \"ðŸ”µ STANDARD: General Queue\"\n",
        "    }\n",
        "\n",
        "    return pred, actions.get(pred, \"Review\")\n",
        "\n",
        "# --- LIVE TEST ---\n",
        "test_messages = [\n",
        "    \"I have been waiting for 3 hours! This is ridiculous.\",\n",
        "    \"I love the new update, great job team!\",\n",
        "    \"Can you tell me how to reset my password?\",\n",
        "    \"The app crashes when I click the blue button.\",\n",
        "    \"I am never using this service again. Refund me now.\"\n",
        "]\n",
        "\n",
        "print(\"---!!! SIMULATION !!!---\")\n",
        "for msg in test_messages:\n",
        "    priority, action = predict_priority(msg)\n",
        "    print(f\"Input: \\\"{msg}\\\"\")\n",
        "    print(f\"Detected: {priority}\")\n",
        "    print(f\"Action: {action}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P94nTz_NfT4u"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))\n",
        "print(\"TEST Macro-F1:\", f1_score(y_test, y_pred, average='macro'))\n",
        "print(\"TEST Weighted-F1:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"Angry Recall (Risk Catch Rate):\", angry_recall)\n",
        "print(\"TEST Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}